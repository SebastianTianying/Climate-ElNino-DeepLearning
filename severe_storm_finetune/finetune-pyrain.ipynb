{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8795d1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The main goal is to finetune ClimaX on either ERA5 or ERA5 + Nino data in order to predict severe storms. We then compare their output to see if adding El Nino benefits severe storm prediction.\n",
    "\n",
    "![baseline.png](images/baseline.png)\n",
    "\n",
    "In order to get our data in the proper format follow the colab notebook [here](https://colab.research.google.com/drive/1eVJUQkIUGvwLQj-TU5UYvGBR02monomp?usp=sharing), or in data_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d691b15",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Create a conda environment for training ClimaX. Installation guide can be found [here](https://microsoft.github.io/ClimaX/install/). This notebook also requires installing additional packages:\n",
    "\n",
    "```\n",
    "pip install dill ## for loading pyrain data\n",
    "pip install deepspeed ## for efficient training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs\n",
    "\n",
    "We can use a hparams dictionary to store all the hyperparameters for initializing the dataloaders and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'seed': 2020,\n",
    "    'sources': 'era_nino',  # options: 'era', 'nino', 'era_nino'\n",
    "    'imerg': True,\n",
    "    'grid': 5.625, \n",
    "    'time_history': 3,\n",
    "    'sample_time_window': 12,\n",
    "    'sample_freq': 6,\n",
    "    'forecast_time_window': 4384,\n",
    "    'forecast_freq': 2192,\n",
    "    'inc_time': True,\n",
    "    'data_paths': [  # where precipitation data is stored\n",
    "        '/localhome/data/datasets/severestorms/era5625_aaai/era5625_us.dill', \n",
    "        '/localhome/data/datasets/severestorms/storm5625/storm_data_us.dill', \n",
    "        '/localhome/data/datasets/severestorms/nino34_5625/nino34_5625.dill'\n",
    "    ],\n",
    "    'norm_path': 'pyrain/normalize.json',\n",
    "    'log_path': '/localhome/data/ckpts/',  # where the checkpoints should go\n",
    "    'gpus': 1,\n",
    "    'use_amp': True,\n",
    "    'batch_size': 32,\n",
    "    'lr': 5e-05,\n",
    "    'num_workers': 4,\n",
    "    # 'strategy': 'deepspeed_stage_2', # Deepspeed not available in interactive environments\n",
    "    'strategy': None,\n",
    "    'acc_grad': 1,\n",
    "    'version': 'pyrain-finetune-template',\n",
    "    'plot': False,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'weight_decay': 1e-05,\n",
    "    'warmup_epochs': 5,\n",
    "    'max_epochs': 20,\n",
    "    'warmup_start_lr': 1e-08,\n",
    "    'eta_min': 0.00000001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datamodule\n",
    "\n",
    "We will first define a datamodule that will load the data and prepare it for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/climaX-finetune/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-07 18:19:33,347] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pyrain.dataset import SevereStormDataset\n",
    "from pyrain.collect_data import write_data_config, read_normalization_stats\n",
    "from pyrain.utils import get_local_shift, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SevereStormDataModule(LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data_dir, self.partition_conf, self.sample_conf = write_data_config(hparams)\n",
    "        self.normalizer = read_normalization_stats(hparams['norm_path'])\n",
    "\n",
    "        self.train_dataset = SevereStormDataset(\n",
    "            datapath=self.data_dir,\n",
    "            partition_conf=self.partition_conf,\n",
    "            partition_type=\"range\",\n",
    "            partition_selected=\"train\",\n",
    "            sample_conf=self.sample_conf)\n",
    "\n",
    "        self.val_dataset = SevereStormDataset(\n",
    "            datapath=self.data_dir,\n",
    "            partition_conf=self.partition_conf,\n",
    "            partition_type=\"range\",\n",
    "            partition_selected=\"valid\",\n",
    "            sample_conf=self.sample_conf)\n",
    "\n",
    "        self.test_dataset = SevereStormDataset(\n",
    "            datapath=self.data_dir,\n",
    "            partition_conf=self.partition_conf,\n",
    "            partition_type=\"range\",\n",
    "            partition_selected=\"test\",\n",
    "            sample_conf=self.sample_conf)\n",
    "\n",
    "        time_shift = None\n",
    "        if hparams['inc_time']:\n",
    "            time_shift = get_local_shift(hparams['grid'], self.train_dataset.dataset)\n",
    "        self.collate = lambda x: collate_fn(x, hparams, self.normalizer, time_shift)\n",
    "\n",
    "    \n",
    "\n",
    "    def get_lat2d(self, grid):\n",
    "        if grid == 5.625:\n",
    "            lat2d = self.val_dataset.dataset['era5625/lat2d']\n",
    "        else:\n",
    "            lat = np.linspace(-89.296875, 89.296875, 128)\n",
    "            lat2d = np.expand_dims(lat, axis=1).repeat(256, 1)\n",
    "        return lat2d\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], collate_fn=self.collate, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], collate_fn=self.collate, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], collate_fn=self.collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataModule\n",
    "datamodule = SevereStormDataModule()\n",
    "lat2d = datamodule.get_lat2d(hparams['grid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climax.arch import ClimaX\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from climax.utils.pos_embed import get_1d_sincos_pos_embed_from_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimaXSevereStorm(ClimaX):\n",
    "    def __init__(\n",
    "        self,\n",
    "        default_vars,\n",
    "        out_vars,\n",
    "        img_size=[5, 12],\n",
    "        time_history=1,\n",
    "        patch_size=1,\n",
    "        embed_dim=1024,\n",
    "        depth=8,\n",
    "        decoder_depth=2,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        drop_path=0.1,\n",
    "        drop_rate=0.1,\n",
    "        parallel_patch_embed=False,\n",
    "        freeze_encoder=False,\n",
    "    ):\n",
    "        assert out_vars is not None\n",
    "\n",
    "        super().__init__(\n",
    "            default_vars,\n",
    "            img_size,\n",
    "            patch_size,\n",
    "            embed_dim,\n",
    "            depth,\n",
    "            decoder_depth,\n",
    "            num_heads,\n",
    "            mlp_ratio,\n",
    "            drop_path,\n",
    "            drop_rate,\n",
    "            parallel_patch_embed\n",
    "        )\n",
    "\n",
    "        self.out_vars = out_vars\n",
    "        self.time_history = time_history\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "\n",
    "        # used to aggregate multiple timesteps in the input\n",
    "        self.time_pos_embed = nn.Parameter(torch.zeros(1, time_history, embed_dim), requires_grad=True)\n",
    "        self.time_agg = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.time_query = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "\n",
    "        # initialize time embedding\n",
    "        time_pos_embed = get_1d_sincos_pos_embed_from_grid(self.time_pos_embed.shape[-1], np.arange(self.time_history))\n",
    "        self.time_pos_embed.data.copy_(torch.from_numpy(time_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # overwrite ClimaX\n",
    "        # replace head for this task, since output is different\n",
    "        self.head = nn.ModuleList()\n",
    "        for _ in range(decoder_depth):\n",
    "            self.head.append(nn.Linear(embed_dim, embed_dim))\n",
    "            self.head.append(nn.GELU())\n",
    "        self.head.append(nn.Linear(embed_dim, patch_size**2))\n",
    "        self.head = nn.Sequential(*self.head)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for name, p in self.blocks.named_parameters():\n",
    "                name = name.lower()\n",
    "                # we do not freeze the norm layers, as suggested by https://arxiv.org/abs/2103.05247\n",
    "                if 'norm' in name:\n",
    "                    continue\n",
    "                else:\n",
    "                    p.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward_encoder(self, x: torch.Tensor, lead_times: torch.Tensor, variables):\n",
    "        # x: `[B, T, V, H, W]` shape\n",
    "        if isinstance(variables, list):\n",
    "            variables = tuple(variables)\n",
    "        \n",
    "        b, t, _, _, _ = x.shape\n",
    "        x = x.flatten(0, 1)  # BxT, V, H, W\n",
    "        \n",
    "        # tokenize each variable separately\n",
    "        embeds = []\n",
    "        var_ids = self.get_var_ids(variables, x.device)\n",
    "\n",
    "        if self.parallel_patch_embed:\n",
    "            x = self.token_embeds(x, var_ids)  # BxT, V, L, D\n",
    "        else:\n",
    "            for i in range(len(var_ids)):\n",
    "                id = var_ids[i]\n",
    "                embeds.append(self.token_embeds[id](x[:, i : i + 1]))\n",
    "            x = torch.stack(embeds, dim=1)  # BxT, V, L, D\n",
    "\n",
    "        # add variable embedding\n",
    "        var_embed = self.get_var_emb(self.var_embed, variables)\n",
    "        x = x + var_embed.unsqueeze(2)  # BxT, V, L, D\n",
    "\n",
    "        # variable aggregation\n",
    "        x = self.aggregate_variables(x)  # BxT, L, D\n",
    "\n",
    "        # add pos embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # add time embedding\n",
    "        # time emb: 1, T, D\n",
    "        x = x.unflatten(0, sizes=(b, t)) # B, T, L, D\n",
    "        x = x + self.time_pos_embed.unsqueeze(2)\n",
    "\n",
    "        # add lead time embedding\n",
    "        lead_time_emb = self.lead_time_embed(lead_times.unsqueeze(-1)) # B, D\n",
    "        lead_time_emb = lead_time_emb.unsqueeze(1).unsqueeze(2)\n",
    "        x = x + lead_time_emb # B, T, L, D\n",
    "\n",
    "        x = x.flatten(0, 1)  # BxT, L, D\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x) # BxT, L, D  \n",
    "        x = x.unflatten(0, sizes=(b, t)) # B, T, L, D\n",
    "\n",
    "        time_query = self.time_query.repeat_interleave(x.shape[0], dim=0)\n",
    "        # run time_agg for each L, so that the final output is B, L, D\n",
    "        agg_x = torch.empty(0, dtype=x.dtype).to(x.device)\n",
    "        for i in range(x.shape[2]):\n",
    "            agg_x_i, _ = self.time_agg(time_query, x[:, :, i, :], x[:, :, i, :])\n",
    "            agg_x = torch.cat((agg_x, agg_x_i), dim=1)\n",
    "\n",
    "        return agg_x    # B, L, D\n",
    "\n",
    "\n",
    "    def unpatchify(self, x: torch.Tensor, h=None, w=None):\n",
    "        \"\"\"\n",
    "        x: (B, L, patch_size**2)\n",
    "        return imgs: (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        c = 1\n",
    "        h = self.img_size[0] // p if h is None else h // p\n",
    "        w = self.img_size[1] // p if w is None else w // p\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n",
    "        return imgs\n",
    "    \n",
    "\n",
    "    def forward(self, x, y, lead_times, variables, out_variables, metric, lat):\n",
    "        out_transformers = self.forward_encoder(x, lead_times, variables)  # B, L, D\n",
    "        preds = self.head(out_transformers)  # B, L, p*p\n",
    "        preds = self.unpatchify(preds) # B, 1, H, W\n",
    "\n",
    "        if metric is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = [m(preds, y, out_variables, lat) for m in metric]\n",
    "\n",
    "        return loss, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "climax_var_order = [\"lsm\", \"orography\", \"lat2d\", \"t2m\", \"z-500\", \"z-850\", \"t-500\", \"t-850\", \"q-500\", \"q-850\"]\n",
    "categories = hparams['categories']\n",
    "sorted_vars = sorted(categories['input'], key=lambda x: climax_var_order.index(x) if x in climax_var_order else len(climax_var_order))\n",
    "sorted_vars.append('lead_time')\n",
    "net = ClimaXSevereStorm(\n",
    "    default_vars=sorted_vars,\n",
    "    out_vars=categories['output'],\n",
    "    time_history=3, # the number of input timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained weights\n",
    "\n",
    "Load climax pretrained on CMIP6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CMIP6 pretrained checkpoint from https://huggingface.co/tungnd/climax/resolve/main/5.625deg.ckpt\n",
      "Removing key pos_embed from pretrained checkpoint\n",
      "Removing key token_embeds.0.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.1.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.2.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.3.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.4.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.5.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.6.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.7.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.8.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.9.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.10.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.11.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.12.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.13.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.14.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.15.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.16.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.17.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.18.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.19.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.20.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.21.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.22.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.23.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.24.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.25.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.26.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.26.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.27.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.27.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.28.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.28.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.29.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.29.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.30.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.30.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.31.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.31.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.32.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.32.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.33.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.33.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.34.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.34.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.35.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.35.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.36.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.36.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.37.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.37.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.38.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.38.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.39.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.39.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.40.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.40.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.41.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.41.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.42.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.42.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.43.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.43.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.44.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.44.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.45.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.45.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.46.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.46.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.47.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.47.proj.bias from pretrained checkpoint\n",
      "Removing key head.4.weight from pretrained checkpoint\n",
      "Removing key head.4.bias from pretrained checkpoint\n",
      "Removing key var_embed from pretrained checkpoint\n",
      "_IncompatibleKeys(missing_keys=['var_embed', 'pos_embed', 'time_pos_embed', 'time_query', 'token_embeds.0.proj.weight', 'token_embeds.1.proj.weight', 'token_embeds.2.proj.weight', 'token_embeds.3.proj.weight', 'token_embeds.4.proj.weight', 'token_embeds.5.proj.weight', 'token_embeds.6.proj.weight', 'token_embeds.7.proj.weight', 'token_embeds.8.proj.weight', 'token_embeds.9.proj.weight', 'token_embeds.10.proj.weight', 'token_embeds.11.proj.weight', 'token_embeds.12.proj.weight', 'token_embeds.13.proj.weight', 'token_embeds.14.proj.weight', 'token_embeds.15.proj.weight', 'token_embeds.16.proj.weight', 'token_embeds.17.proj.weight', 'token_embeds.18.proj.weight', 'token_embeds.19.proj.weight', 'token_embeds.20.proj.weight', 'token_embeds.21.proj.weight', 'token_embeds.22.proj.weight', 'token_embeds.23.proj.weight', 'token_embeds.24.proj.weight', 'token_embeds.25.proj.weight', 'head.4.weight', 'head.4.bias', 'time_agg.in_proj_weight', 'time_agg.in_proj_bias', 'time_agg.out_proj.weight', 'time_agg.out_proj.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "from climax.loaders import load_climax\n",
    "\n",
    "net = load_climax(\n",
    "    version=1,\n",
    "    pretraining_data='cmip6',\n",
    "    resolution=hparams['grid'],\n",
    "    preset_net=net\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ERA5 + Nino dataset has 22 variables, 17 coming from atmospheric ERA5 variables, 4 from El nino indices, and 1 for lead time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Module\n",
    "\n",
    "We will now define the forecast module that will be used for training. This module will be initialized with the pretrained climaX model and will be finetuned on the precipitation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "from climax.utils.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from climax.utils.metrics import (\n",
    "    mse,\n",
    "    lat_weighted_mse,\n",
    "    lat_weighted_nrmse, \n",
    "    lat_weighted_rmse,\n",
    ")\n",
    "from climax.utils.pos_embed import interpolate_pos_embed\n",
    "from typing import Any\n",
    "from pyrain.metrics import eval_loss, define_loss_fn, collect_outputs\n",
    "from deepspeed.ops import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StormForecastModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net,\n",
    "        lat2d,\n",
    "        normalizer,\n",
    "        pretrained_path: str = \"\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.categories = hparams['categories']\n",
    "        self.net = net\n",
    "        if len(pretrained_path) > 0:\n",
    "            self.load_pretrained_weights(pretrained_path)\n",
    "\n",
    "        self.lead_times = hparams['lead_times']\n",
    "        self.lat, self.lon = hparams['latlon']\n",
    "        self.test_step_outputs = []\n",
    "        self.val_step_outputs = []\n",
    "        self.version = hparams[\"version\"]\n",
    "        self.normalizer = normalizer\n",
    "        \n",
    "        self.weights_lat, self.loss = define_loss_fn(lat2d)\n",
    "        self.lat = lat2d[0][:,0]\n",
    "        self.predictions = []\n",
    "\n",
    "    def load_pretrained_weights(self, pretrained_path):\n",
    "        if pretrained_path.startswith(\"http\"):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(pretrained_path, map_location=torch.device(\"cpu\"))\n",
    "        else:\n",
    "            checkpoint = torch.load(pretrained_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "        print(\"Loading pre-trained checkpoint from: %s\" % pretrained_path)\n",
    "        checkpoint_model = checkpoint[\"state_dict\"]\n",
    "        # interpolate positional embedding\n",
    "        interpolate_pos_embed(self.net, checkpoint_model, new_size=self.net.img_size)\n",
    "\n",
    "        state_dict = self.state_dict()\n",
    "        if self.net.parallel_patch_embed:\n",
    "            if \"token_embeds.proj_weights\" not in checkpoint_model.keys():\n",
    "                raise ValueError(\n",
    "                    \"Pretrained checkpoint does not have token_embeds.proj_weights for parallel processing. Please convert the checkpoints first or disable parallel patch_embed tokenization.\"\n",
    "                )\n",
    "\n",
    "        for k in list(checkpoint_model.keys()):\n",
    "            if \"channel\" in k:\n",
    "                checkpoint_model[k.replace(\"channel\", \"var\")] = checkpoint_model[k]\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "            if \"head\" in k:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint.\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        for k in list(checkpoint_model.keys()):\n",
    "            if k not in state_dict.keys() or checkpoint_model[k].shape != state_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        # load pre-trained model\n",
    "        msg = self.load_state_dict(checkpoint_model, strict=False)\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        x, y, lead_times = batch\n",
    "\n",
    "        loss_dict, p = self.net.forward(x, y, lead_times, self.categories['input'], self.categories['output'], [lat_weighted_mse], lat=self.lat)\n",
    "\n",
    "        loss_dict = loss_dict[0]\n",
    "        for var in loss_dict.keys():\n",
    "            self.log(\n",
    "                \"train/\" + var,\n",
    "                loss_dict[var],\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "            )\n",
    "        loss = loss_dict['loss']\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        x, y, lead_times = batch\n",
    "\n",
    "        _, pred = self.net.forward(\n",
    "            x,\n",
    "            y,\n",
    "            lead_times,\n",
    "            self.categories['input'],\n",
    "            self.categories['output'],\n",
    "            metric=None,\n",
    "            lat=self.lat,\n",
    "        )\n",
    "\n",
    "        results = eval_loss(pred, y, lead_times, self.loss, self.lead_times, phase='val', target_v=self.categories['output'][0], normalizer=self.normalizer)\n",
    "\n",
    "        self.val_step_outputs.append(results)\n",
    "        return results\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        node_loss = collect_outputs(self.val_step_outputs, False)\n",
    "        self.val_step_outputs.clear()  # free memory\n",
    "\n",
    "        if isinstance(node_loss, list):\n",
    "            node_loss = node_loss[0]\n",
    "    \n",
    "        all_losses = self.all_gather(node_loss)\n",
    "        mean_losses = {k: float(torch.mean(x)) for k, x in all_losses.items()}\n",
    "\n",
    "        # log mean losses\n",
    "        for var in mean_losses.keys():\n",
    "            self.log(\n",
    "                \"val/\" + var,\n",
    "                mean_losses[var],\n",
    "                sync_dist=True\n",
    "            )\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        x, y, lead_times = batch\n",
    "        _, pred = self.net.forward(\n",
    "            x,\n",
    "            y,\n",
    "            lead_times,\n",
    "            self.categories['input'],\n",
    "            self.categories['output'],\n",
    "            metric=None,\n",
    "            lat=self.lat,\n",
    "        )\n",
    "        results = eval_loss(pred, y, lead_times, self.loss, self.lead_times, phase='test', target_v=self.categories['output'][0], normalizer=self.normalizer)\n",
    "        self.predictions.append(pred.detach().cpu())\n",
    "        self.test_step_outputs.append(results)\n",
    "        return results\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        node_loss = collect_outputs(self.test_step_outputs, False)\n",
    "        self.test_step_outputs.clear()  # free memory\n",
    "\n",
    "        if isinstance(node_loss, list):\n",
    "            node_loss = node_loss[0]\n",
    "    \n",
    "        all_losses = self.all_gather(node_loss)\n",
    "        mean_losses = {k: float(torch.mean(x)) for k, x in all_losses.items()}\n",
    "\n",
    "        # log mean losses\n",
    "        for var in mean_losses.keys():\n",
    "            self.log(\n",
    "                \"test/\" + var,\n",
    "                mean_losses[var],\n",
    "                sync_dist=True\n",
    "            )\n",
    "        \n",
    "        # Save evaluation results\n",
    "        results_path = Path(f'./results/{self.version}_results.json')\n",
    "        \n",
    "        with open(results_path, 'w') as fp:\n",
    "            json.dump(mean_losses, fp, indent=4)\n",
    "\n",
    "        fp.close()\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        decay = []\n",
    "        no_decay = []\n",
    "        for name, m in self.named_parameters():\n",
    "            if \"var_embed\" in name or \"pos_embed\" in name or \"time_pos_embed\" in name:\n",
    "                no_decay.append(m)\n",
    "            else:\n",
    "                decay.append(m)\n",
    "\n",
    "        optimizer = adam.FusedAdam(\n",
    "            [\n",
    "                {\n",
    "                    \"params\": decay,\n",
    "                    \"lr\": hparams['lr'],\n",
    "                    \"betas\": (hparams['beta_1'], hparams['beta_2']),\n",
    "                    \"weight_decay\": hparams['weight_decay'],\n",
    "                },\n",
    "                {\n",
    "                    \"params\": no_decay,\n",
    "                    \"lr\": hparams['lr'],\n",
    "                    \"betas\": (hparams['beta_1'], hparams['beta_2']),\n",
    "                    \"weight_decay\": 0\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        lr_scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer,\n",
    "            hparams['warmup_epochs'],\n",
    "            hparams['max_epochs'],\n",
    "            hparams['warmup_start_lr'],\n",
    "            hparams['eta_min'],\n",
    "        )\n",
    "        scheduler = {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StormForecastModule(net, lat2d, datamodule.normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Train the model using the datamodule and storm forecast module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# define logger\n",
    "Path(hparams['log_path']).mkdir(parents=True, exist_ok=True)\n",
    "logger = loggers.TensorBoardLogger(hparams['log_path'], version=hparams['version'])\n",
    "logger.log_hyperparams(params=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=hparams['gpus'],\n",
    "    logger=logger,\n",
    "    max_epochs=hparams['max_epochs'],\n",
    "    precision=16 if hparams['use_amp'] else 32,\n",
    "    default_root_dir=hparams['log_path'],\n",
    "    strategy=hparams['strategy'],\n",
    "    callbacks=[\n",
    "        EarlyStopping('val/val_loss', patience=5), \n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        ModelCheckpoint(\n",
    "            dirpath='{}/{}/'.format(hparams['log_path'], hparams['version']),\n",
    "            filename='epoch-{epoch:03d}',\n",
    "            monitor='val/val_loss',\n",
    "            save_top_k=1,\n",
    "            mode='min',\n",
    "            save_last=True,\n",
    "            verbose=False,\n",
    "            auto_insert_metric_name=False,\n",
    "        )\n",
    "    ],\n",
    "    accumulate_grad_batches=hparams['acc_grad'],\n",
    ")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate result on last checkpoint from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = trainer.test(model.cuda(), ckpt_path='/home/allen/ckpts/pyrain-finetune-template/last.ckpt', datamodule=datamodule) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a09d01",
   "metadata": {},
   "source": [
    "### Results\n",
    "The resulting MSE of all experiments\n",
    "\n",
    "\n",
    "![results.png](images/results.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afb56250527054819fcf9d138637959b1d6f2cc63b50ebf691b81e000c3d0b62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
