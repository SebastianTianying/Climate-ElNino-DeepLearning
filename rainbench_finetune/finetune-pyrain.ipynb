{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Pyrain\n",
    "\n",
    "This notebook will demonstrate how to finetune climaX on the RainBench preciptation data and code provided by [Pyrain](https://github.com/FrontierDevelopmentLab/PyRain/tree/master). This notebook is based on the [Pytorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/) framework and can be adapted to other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "The precipitation data provided by Pyrain can be downloaded [here](https://console.cloud.google.com/storage/browser/aaai_release).\n",
    "\n",
    "### Input\n",
    "For this notebook, we will use RainBench data from both Simsat and ERA5. Refer to the [Rainbench](https://arxiv.org/abs/2012.09670) paper for the complete list of variables. The input is composed of time series over a 12 hour period, sampled every 6 hours by default.\n",
    "\n",
    "![input.png](images/input.png)\n",
    "\n",
    "The shape is $T \\times V \\times H \\times W$, where $T$ is the number of input time steps, $V$ is the number of variables, and $H$, $W$ the spatial resolution (32 x 64 for 5.625°).\n",
    "\n",
    "### Output\n",
    "The network will be trained to predict the precipitation at several lead times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Create a conda environment for training ClimaX. Installation guide can be found [here](https://microsoft.github.io/ClimaX/install/). This notebook also requires installing additional packages:\n",
    "\n",
    "```bash\n",
    "pip install dill ## for loading pyrain data\n",
    "pip install deepspeed ## for efficient training\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs\n",
    "\n",
    "We can use a hparams dictionary to store all the hyperparameters for initializing the dataloaders and training. Remeber to change the paths for `data_paths` and `log_path`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'seed': 2020,\n",
    "    'sources': 'era_nino',  # options: 'simsat_era', 'simsat', 'era', 'era16_3'\n",
    "    'imerg': True,  # options: True (predict IMERG), False (predict ERA5)\n",
    "    'grid': 5.625, \n",
    "    'time_history': 3,\n",
    "    'sample_time_window': 12,\n",
    "    'sample_freq': 6,\n",
    "    'forecast_time_window': 4384,\n",
    "    'forecast_freq': 2192,\n",
    "    'inc_time': True,\n",
    "    'data_paths': [  # where precipitation data is stored\n",
    "        '/home/allen/data/rainbench/era5625_aaai/era5625_us.dill', \n",
    "        '/home/allen/data/rainbench/imerg5625/storm_data_us.dill', \n",
    "        '/home/allen/data/rainbench/simsat5625/simsat5625.dill',\n",
    "        '/home/allen/data/rainbench/nino34_5625/nino34_5625.dill'\n",
    "    ],\n",
    "    'norm_path': 'pyrain/normalize.json',\n",
    "    'log_path': '/home/allen/ckpts/',  # where the checkpoints should go\n",
    "    'gpus': 1,\n",
    "    'use_amp': True,\n",
    "    'batch_size': 32,\n",
    "    'lr': 5e-05,\n",
    "    'num_workers': 4,\n",
    "    # 'strategy': 'deepspeed_stage_2', # Deepspeed not available in interactive environments\n",
    "    'strategy': None,\n",
    "    'acc_grad': 1,\n",
    "    'version': 'pyrain-finetune-template',\n",
    "    'plot': False,\n",
    "    'beta_1': 0.9,\n",
    "    'beta_2': 0.999,\n",
    "    'weight_decay': 1e-05,\n",
    "    'warmup_epochs': 5,\n",
    "    'max_epochs': 20,\n",
    "    'warmup_start_lr': 1e-08,\n",
    "    'eta_min': 0.00000001\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datamodule\n",
    "\n",
    "We will first define a datamodule that will load the data and prepare it for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/climaX-finetune/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-07 18:19:33,347] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pyrain.dataset import RainbenchDataset\n",
    "from pyrain.collect_data import write_data_config, read_normalization_stats\n",
    "from pyrain.utils import get_local_shift, collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbenchDataModule(LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data_dir, self.partition_conf, self.sample_conf = write_data_config(hparams)\n",
    "        self.normalizer = read_normalization_stats(hparams['norm_path'])\n",
    "\n",
    "        self.train_dataset = RainbenchDataset(\n",
    "            datapath=self.data_dir,\n",
    "            partition_conf=self.partition_conf,\n",
    "            partition_type=\"range\",\n",
    "            partition_selected=\"train\",\n",
    "            sample_conf=self.sample_conf)\n",
    "\n",
    "        self.val_dataset = RainbenchDataset(\n",
    "            datapath=self.data_dir,\n",
    "            partition_conf=self.partition_conf,\n",
    "            partition_type=\"range\",\n",
    "            partition_selected=\"valid\",\n",
    "            sample_conf=self.sample_conf)\n",
    "\n",
    "        self.test_dataset = RainbenchDataset(\n",
    "            datapath=self.data_dir,\n",
    "            partition_conf=self.partition_conf,\n",
    "            partition_type=\"range\",\n",
    "            partition_selected=\"test\",\n",
    "            sample_conf=self.sample_conf)\n",
    "\n",
    "        time_shift = None\n",
    "        if hparams['inc_time']:\n",
    "            time_shift = get_local_shift(hparams['grid'], self.train_dataset.dataset)\n",
    "        self.collate = lambda x: collate_fn(x, hparams, self.normalizer, time_shift)\n",
    "\n",
    "    \n",
    "\n",
    "    def get_lat2d(self, grid):\n",
    "        if grid == 5.625:\n",
    "            lat2d = self.val_dataset.dataset['era5625/lat2d']\n",
    "        else:\n",
    "            lat = np.linspace(-89.296875, 89.296875, 128)\n",
    "            lat2d = np.expand_dims(lat, axis=1).repeat(256, 1)\n",
    "        return lat2d\n",
    "            \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], collate_fn=self.collate, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], collate_fn=self.collate, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=hparams['batch_size'], num_workers=hparams['num_workers'], collate_fn=self.collate, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define DataModule\n",
    "datamodule = RainbenchDataModule()\n",
    "lat2d = datamodule.get_lat2d(hparams['grid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture\n",
    "\n",
    "We need to modify the original ClimaX architecture for this task because:\n",
    "- We want to predict only precipitation, while the original architecture predicts all input variables.\n",
    "- We must aggregate multiple time steps into a single input and add a time embedding to the input. This was not taken into account by the original model\n",
    "\n",
    "We base this new architecture on the original architecture.\n",
    "\n",
    "<!-- Since the [input](#Input) and [output](#Output) are different from the original climaX task, we will need to modify the model architecture. We will use the same encoder and decoder (no freezing), but change the head to predict just one variable (precipitation) at a time. We also must aggregate multiple time steps into a single input and add a time embedding to the input. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climax.arch import ClimaX\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from climax.utils.pos_embed import get_1d_sincos_pos_embed_from_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimaXRainBench(ClimaX):\n",
    "    def __init__(\n",
    "        self,\n",
    "        default_vars,\n",
    "        out_vars,\n",
    "        img_size=[5, 12],\n",
    "        time_history=1,\n",
    "        patch_size=1,\n",
    "        embed_dim=1024,\n",
    "        depth=8,\n",
    "        decoder_depth=2,\n",
    "        num_heads=16,\n",
    "        mlp_ratio=4.0,\n",
    "        drop_path=0.1,\n",
    "        drop_rate=0.1,\n",
    "        parallel_patch_embed=False,\n",
    "        freeze_encoder=False,\n",
    "    ):\n",
    "        assert out_vars is not None\n",
    "\n",
    "        super().__init__(\n",
    "            default_vars,\n",
    "            img_size,\n",
    "            patch_size,\n",
    "            embed_dim,\n",
    "            depth,\n",
    "            decoder_depth,\n",
    "            num_heads,\n",
    "            mlp_ratio,\n",
    "            drop_path,\n",
    "            drop_rate,\n",
    "            parallel_patch_embed\n",
    "        )\n",
    "\n",
    "        self.out_vars = out_vars\n",
    "        self.time_history = time_history\n",
    "        self.freeze_encoder = freeze_encoder\n",
    "\n",
    "        # used to aggregate multiple timesteps in the input\n",
    "        self.time_pos_embed = nn.Parameter(torch.zeros(1, time_history, embed_dim), requires_grad=True)\n",
    "        self.time_agg = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.time_query = nn.Parameter(torch.zeros(1, 1, embed_dim), requires_grad=True)\n",
    "\n",
    "        # initialize time embedding\n",
    "        time_pos_embed = get_1d_sincos_pos_embed_from_grid(self.time_pos_embed.shape[-1], np.arange(self.time_history))\n",
    "        self.time_pos_embed.data.copy_(torch.from_numpy(time_pos_embed).float().unsqueeze(0))\n",
    "\n",
    "        # overwrite ClimaX\n",
    "        # replace head for this task, since output is different\n",
    "        self.head = nn.ModuleList()\n",
    "        for _ in range(decoder_depth):\n",
    "            self.head.append(nn.Linear(embed_dim, embed_dim))\n",
    "            self.head.append(nn.GELU())\n",
    "        self.head.append(nn.Linear(embed_dim, patch_size**2))\n",
    "        self.head = nn.Sequential(*self.head)\n",
    "\n",
    "        if freeze_encoder:\n",
    "            for name, p in self.blocks.named_parameters():\n",
    "                name = name.lower()\n",
    "                # we do not freeze the norm layers, as suggested by https://arxiv.org/abs/2103.05247\n",
    "                if 'norm' in name:\n",
    "                    continue\n",
    "                else:\n",
    "                    p.requires_grad_(False)\n",
    "\n",
    "\n",
    "    def forward_encoder(self, x: torch.Tensor, lead_times: torch.Tensor, variables):\n",
    "        # x: `[B, T, V, H, W]` shape\n",
    "        if isinstance(variables, list):\n",
    "            variables = tuple(variables)\n",
    "        \n",
    "        b, t, _, _, _ = x.shape\n",
    "        x = x.flatten(0, 1)  # BxT, V, H, W\n",
    "        \n",
    "        # tokenize each variable separately\n",
    "        embeds = []\n",
    "        var_ids = self.get_var_ids(variables, x.device)\n",
    "\n",
    "        if self.parallel_patch_embed:\n",
    "            x = self.token_embeds(x, var_ids)  # BxT, V, L, D\n",
    "        else:\n",
    "            for i in range(len(var_ids)):\n",
    "                id = var_ids[i]\n",
    "                embeds.append(self.token_embeds[id](x[:, i : i + 1]))\n",
    "            x = torch.stack(embeds, dim=1)  # BxT, V, L, D\n",
    "\n",
    "        # add variable embedding\n",
    "        var_embed = self.get_var_emb(self.var_embed, variables)\n",
    "        x = x + var_embed.unsqueeze(2)  # BxT, V, L, D\n",
    "\n",
    "        # variable aggregation\n",
    "        x = self.aggregate_variables(x)  # BxT, L, D\n",
    "\n",
    "        # add pos embedding\n",
    "        x = x + self.pos_embed\n",
    "\n",
    "        # add time embedding\n",
    "        # time emb: 1, T, D\n",
    "        x = x.unflatten(0, sizes=(b, t)) # B, T, L, D\n",
    "        x = x + self.time_pos_embed.unsqueeze(2)\n",
    "\n",
    "        # add lead time embedding\n",
    "        lead_time_emb = self.lead_time_embed(lead_times.unsqueeze(-1)) # B, D\n",
    "        lead_time_emb = lead_time_emb.unsqueeze(1).unsqueeze(2)\n",
    "        x = x + lead_time_emb # B, T, L, D\n",
    "\n",
    "        x = x.flatten(0, 1)  # BxT, L, D\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        # apply Transformer blocks\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x) # BxT, L, D  \n",
    "        x = x.unflatten(0, sizes=(b, t)) # B, T, L, D\n",
    "\n",
    "        time_query = self.time_query.repeat_interleave(x.shape[0], dim=0)\n",
    "        # run time_agg for each L, so that the final output is B, L, D\n",
    "        agg_x = torch.empty(0, dtype=x.dtype).to(x.device)\n",
    "        for i in range(x.shape[2]):\n",
    "            agg_x_i, _ = self.time_agg(time_query, x[:, :, i, :], x[:, :, i, :])\n",
    "            agg_x = torch.cat((agg_x, agg_x_i), dim=1)\n",
    "\n",
    "        return agg_x    # B, L, D\n",
    "\n",
    "\n",
    "    def unpatchify(self, x: torch.Tensor, h=None, w=None):\n",
    "        \"\"\"\n",
    "        x: (B, L, patch_size**2)\n",
    "        return imgs: (B, 1, H, W)\n",
    "        \"\"\"\n",
    "        p = self.patch_size\n",
    "        c = 1\n",
    "        h = self.img_size[0] // p if h is None else h // p\n",
    "        w = self.img_size[1] // p if w is None else w // p\n",
    "        assert h * w == x.shape[1]\n",
    "\n",
    "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
    "        x = torch.einsum(\"nhwpqc->nchpwq\", x)\n",
    "        imgs = x.reshape(shape=(x.shape[0], c, h * p, w * p))\n",
    "        return imgs\n",
    "    \n",
    "\n",
    "    def forward(self, x, y, lead_times, variables, out_variables, metric, lat):\n",
    "        out_transformers = self.forward_encoder(x, lead_times, variables)  # B, L, D\n",
    "        preds = self.head(out_transformers)  # B, L, p*p\n",
    "        preds = self.unpatchify(preds) # B, 1, H, W\n",
    "\n",
    "        if metric is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = [m(preds, y, out_variables, lat) for m in metric]\n",
    "\n",
    "        return loss, preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "climax_var_order = [\"lsm\", \"orography\", \"lat2d\", \"t2m\", \"z-500\", \"z-850\", \"t-500\", \"t-850\", \"q-500\", \"q-850\"]\n",
    "categories = hparams['categories']\n",
    "sorted_vars = sorted(categories['input'], key=lambda x: climax_var_order.index(x) if x in climax_var_order else len(climax_var_order))\n",
    "sorted_vars.append('lead_time')\n",
    "net = ClimaXRainBench(\n",
    "    default_vars=sorted_vars,\n",
    "    out_vars=categories['output'],\n",
    "    time_history=3, # the number of input timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained weights\n",
    "\n",
    "We provide several pretrained models that can be used for finetuning on a different task. For more details, please check out the load_climax function.\n",
    "\n",
    "In this particular benchmark, we will load ClimaX-v1 pretrained on CMIP6 at 5.625 deg. Note that we also pass our custom network to this function. This may not be needed for tasks that can use the original ClimaX architecture for finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CMIP6 pretrained checkpoint from https://huggingface.co/tungnd/climax/resolve/main/5.625deg.ckpt\n",
      "Removing key pos_embed from pretrained checkpoint\n",
      "Removing key token_embeds.0.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.1.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.2.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.3.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.4.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.5.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.6.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.7.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.8.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.9.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.10.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.11.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.12.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.13.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.14.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.15.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.16.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.17.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.18.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.19.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.20.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.21.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.22.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.23.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.24.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.25.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.26.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.26.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.27.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.27.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.28.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.28.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.29.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.29.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.30.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.30.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.31.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.31.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.32.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.32.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.33.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.33.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.34.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.34.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.35.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.35.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.36.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.36.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.37.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.37.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.38.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.38.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.39.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.39.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.40.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.40.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.41.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.41.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.42.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.42.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.43.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.43.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.44.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.44.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.45.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.45.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.46.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.46.proj.bias from pretrained checkpoint\n",
      "Removing key token_embeds.47.proj.weight from pretrained checkpoint\n",
      "Removing key token_embeds.47.proj.bias from pretrained checkpoint\n",
      "Removing key head.4.weight from pretrained checkpoint\n",
      "Removing key head.4.bias from pretrained checkpoint\n",
      "Removing key var_embed from pretrained checkpoint\n",
      "_IncompatibleKeys(missing_keys=['var_embed', 'pos_embed', 'time_pos_embed', 'time_query', 'token_embeds.0.proj.weight', 'token_embeds.1.proj.weight', 'token_embeds.2.proj.weight', 'token_embeds.3.proj.weight', 'token_embeds.4.proj.weight', 'token_embeds.5.proj.weight', 'token_embeds.6.proj.weight', 'token_embeds.7.proj.weight', 'token_embeds.8.proj.weight', 'token_embeds.9.proj.weight', 'token_embeds.10.proj.weight', 'token_embeds.11.proj.weight', 'token_embeds.12.proj.weight', 'token_embeds.13.proj.weight', 'token_embeds.14.proj.weight', 'token_embeds.15.proj.weight', 'token_embeds.16.proj.weight', 'token_embeds.17.proj.weight', 'token_embeds.18.proj.weight', 'token_embeds.19.proj.weight', 'token_embeds.20.proj.weight', 'token_embeds.21.proj.weight', 'token_embeds.22.proj.weight', 'token_embeds.23.proj.weight', 'token_embeds.24.proj.weight', 'token_embeds.25.proj.weight', 'head.4.weight', 'head.4.bias', 'time_agg.in_proj_weight', 'time_agg.in_proj_bias', 'time_agg.out_proj.weight', 'time_agg.out_proj.bias'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "from climax.loaders import load_climax\n",
    "\n",
    "net = load_climax(\n",
    "    version=1,\n",
    "    pretraining_data='cmip6',\n",
    "    resolution=hparams['grid'],\n",
    "    preset_net=net\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output tells us the following information:\n",
    "- The last layer of the head was replaced, so its keys were removed from the checkpoint, and new keys were added, hence the IncompatibleKeys.\n",
    "- The SIMSAT + ERA5 dataset only has 26 variables, and the lead_time makes it 27. Therefore, the token_embeds 28 to 47 were removed.\n",
    "- The input variables are different, so the var_embed was replaced.\n",
    "- time_pos_embed, time_query, time_agg were not part of the original ClimaX model, but they were added to take multiple timesteps as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast Module\n",
    "\n",
    "We will now define the forecast module that will be used for training. This module will be initialized with the pretrained climaX model and will be finetuned on the precipitation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning import LightningModule, Trainer, loggers\n",
    "from climax.utils.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from climax.utils.metrics import (\n",
    "    mse,\n",
    "    lat_weighted_mse,\n",
    "    lat_weighted_nrmse, \n",
    "    lat_weighted_rmse,\n",
    ")\n",
    "from climax.utils.pos_embed import interpolate_pos_embed\n",
    "from typing import Any\n",
    "from pyrain.metrics import eval_loss, define_loss_fn, collect_outputs\n",
    "from deepspeed.ops import adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainForecastModule(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        net,\n",
    "        lat2d,\n",
    "        normalizer,\n",
    "        pretrained_path: str = \"\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.categories = hparams['categories']\n",
    "        self.net = net\n",
    "        if len(pretrained_path) > 0:\n",
    "            self.load_pretrained_weights(pretrained_path)\n",
    "\n",
    "        self.lead_times = hparams['lead_times']\n",
    "        self.lat, self.lon = hparams['latlon']\n",
    "        self.test_step_outputs = []\n",
    "        self.val_step_outputs = []\n",
    "        self.version = hparams[\"version\"]\n",
    "        self.normalizer = normalizer\n",
    "        \n",
    "        self.weights_lat, self.loss = define_loss_fn(lat2d)\n",
    "        self.lat = lat2d[0][:,0]\n",
    "        self.predictions = []\n",
    "\n",
    "    def load_pretrained_weights(self, pretrained_path):\n",
    "        if pretrained_path.startswith(\"http\"):\n",
    "            checkpoint = torch.hub.load_state_dict_from_url(pretrained_path, map_location=torch.device(\"cpu\"))\n",
    "        else:\n",
    "            checkpoint = torch.load(pretrained_path, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "        print(\"Loading pre-trained checkpoint from: %s\" % pretrained_path)\n",
    "        checkpoint_model = checkpoint[\"state_dict\"]\n",
    "        # interpolate positional embedding\n",
    "        interpolate_pos_embed(self.net, checkpoint_model, new_size=self.net.img_size)\n",
    "\n",
    "        state_dict = self.state_dict()\n",
    "        if self.net.parallel_patch_embed:\n",
    "            if \"token_embeds.proj_weights\" not in checkpoint_model.keys():\n",
    "                raise ValueError(\n",
    "                    \"Pretrained checkpoint does not have token_embeds.proj_weights for parallel processing. Please convert the checkpoints first or disable parallel patch_embed tokenization.\"\n",
    "                )\n",
    "\n",
    "        for k in list(checkpoint_model.keys()):\n",
    "            if \"channel\" in k:\n",
    "                checkpoint_model[k.replace(\"channel\", \"var\")] = checkpoint_model[k]\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "            if \"head\" in k:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint.\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        for k in list(checkpoint_model.keys()):\n",
    "            if k not in state_dict.keys() or checkpoint_model[k].shape != state_dict[k].shape:\n",
    "                print(f\"Removing key {k} from pretrained checkpoint\")\n",
    "                del checkpoint_model[k]\n",
    "\n",
    "        # load pre-trained model\n",
    "        msg = self.load_state_dict(checkpoint_model, strict=False)\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "    def training_step(self, batch: Any, batch_idx: int):\n",
    "        x, y, lead_times = batch\n",
    "\n",
    "        loss_dict, p = self.net.forward(x, y, lead_times, self.categories['input'], self.categories['output'], [lat_weighted_mse], lat=self.lat)\n",
    "\n",
    "        loss_dict = loss_dict[0]\n",
    "        for var in loss_dict.keys():\n",
    "            self.log(\n",
    "                \"train/\" + var,\n",
    "                loss_dict[var],\n",
    "                on_step=True,\n",
    "                on_epoch=False,\n",
    "                prog_bar=True,\n",
    "            )\n",
    "        loss = loss_dict['loss']\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: Any, batch_idx: int):\n",
    "        x, y, lead_times = batch\n",
    "\n",
    "        _, pred = self.net.forward(\n",
    "            x,\n",
    "            y,\n",
    "            lead_times,\n",
    "            self.categories['input'],\n",
    "            self.categories['output'],\n",
    "            metric=None,\n",
    "            lat=self.lat,\n",
    "        )\n",
    "\n",
    "        results = eval_loss(pred, y, lead_times, self.loss, self.lead_times, phase='val', target_v=self.categories['output'][0], normalizer=self.normalizer)\n",
    "\n",
    "        self.val_step_outputs.append(results)\n",
    "        return results\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        node_loss = collect_outputs(self.val_step_outputs, False)\n",
    "        self.val_step_outputs.clear()  # free memory\n",
    "\n",
    "        if isinstance(node_loss, list):\n",
    "            node_loss = node_loss[0]\n",
    "    \n",
    "        all_losses = self.all_gather(node_loss)\n",
    "        mean_losses = {k: float(torch.mean(x)) for k, x in all_losses.items()}\n",
    "\n",
    "        # log mean losses\n",
    "        for var in mean_losses.keys():\n",
    "            self.log(\n",
    "                \"val/\" + var,\n",
    "                mean_losses[var],\n",
    "                sync_dist=True\n",
    "            )\n",
    "\n",
    "    def test_step(self, batch: Any, batch_idx: int):\n",
    "        x, y, lead_times = batch\n",
    "        _, pred = self.net.forward(\n",
    "            x,\n",
    "            y,\n",
    "            lead_times,\n",
    "            self.categories['input'],\n",
    "            self.categories['output'],\n",
    "            metric=None,\n",
    "            lat=self.lat,\n",
    "        )\n",
    "        results = eval_loss(pred, y, lead_times, self.loss, self.lead_times, phase='test', target_v=self.categories['output'][0], normalizer=self.normalizer)\n",
    "        self.predictions.append(pred.detach().cpu())\n",
    "        self.test_step_outputs.append(results)\n",
    "        return results\n",
    "\n",
    "    def on_test_epoch_end(self) -> None:\n",
    "        node_loss = collect_outputs(self.test_step_outputs, False)\n",
    "        self.test_step_outputs.clear()  # free memory\n",
    "\n",
    "        if isinstance(node_loss, list):\n",
    "            node_loss = node_loss[0]\n",
    "    \n",
    "        all_losses = self.all_gather(node_loss)\n",
    "        mean_losses = {k: float(torch.mean(x)) for k, x in all_losses.items()}\n",
    "\n",
    "        # log mean losses\n",
    "        for var in mean_losses.keys():\n",
    "            self.log(\n",
    "                \"test/\" + var,\n",
    "                mean_losses[var],\n",
    "                sync_dist=True\n",
    "            )\n",
    "        \n",
    "        # Save evaluation results\n",
    "        results_path = Path(f'./results/{self.version}_results.json')\n",
    "        \n",
    "        with open(results_path, 'w') as fp:\n",
    "            json.dump(mean_losses, fp, indent=4)\n",
    "\n",
    "        fp.close()\n",
    "        \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        decay = []\n",
    "        no_decay = []\n",
    "        for name, m in self.named_parameters():\n",
    "            if \"var_embed\" in name or \"pos_embed\" in name or \"time_pos_embed\" in name:\n",
    "                no_decay.append(m)\n",
    "            else:\n",
    "                decay.append(m)\n",
    "\n",
    "        optimizer = adam.FusedAdam(\n",
    "            [\n",
    "                {\n",
    "                    \"params\": decay,\n",
    "                    \"lr\": hparams['lr'],\n",
    "                    \"betas\": (hparams['beta_1'], hparams['beta_2']),\n",
    "                    \"weight_decay\": hparams['weight_decay'],\n",
    "                },\n",
    "                {\n",
    "                    \"params\": no_decay,\n",
    "                    \"lr\": hparams['lr'],\n",
    "                    \"betas\": (hparams['beta_1'], hparams['beta_2']),\n",
    "                    \"weight_decay\": 0\n",
    "                },\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        lr_scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer,\n",
    "            hparams['warmup_epochs'],\n",
    "            hparams['max_epochs'],\n",
    "            hparams['warmup_start_lr'],\n",
    "            hparams['eta_min'],\n",
    "        )\n",
    "        scheduler = {\"scheduler\": lr_scheduler, \"interval\": \"step\", \"frequency\": 1}\n",
    "\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the forecast module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RainForecastModule(net, lat2d, datamodule.normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Finally, we will train the model using the datamodule and forecast module defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# define logger\n",
    "Path(hparams['log_path']).mkdir(parents=True, exist_ok=True)\n",
    "logger = loggers.TensorBoardLogger(hparams['log_path'], version=hparams['version'])\n",
    "logger.log_hyperparams(params=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    accelerator='gpu',\n",
    "    devices=hparams['gpus'],\n",
    "    logger=logger,\n",
    "    max_epochs=hparams['max_epochs'],\n",
    "    precision=16 if hparams['use_amp'] else 32,\n",
    "    default_root_dir=hparams['log_path'],\n",
    "    strategy=hparams['strategy'],\n",
    "    callbacks=[\n",
    "        EarlyStopping('val/val_loss', patience=5), \n",
    "        LearningRateMonitor(logging_interval='step'),\n",
    "        ModelCheckpoint(\n",
    "            dirpath='{}/{}/'.format(hparams['log_path'], hparams['version']),\n",
    "            filename='epoch-{epoch:03d}',\n",
    "            monitor='val/val_loss',\n",
    "            save_top_k=1,\n",
    "            mode='min',\n",
    "            save_last=True,\n",
    "            verbose=False,\n",
    "            auto_insert_metric_name=False,\n",
    "        )\n",
    "    ],\n",
    "    accumulate_grad_batches=hparams['acc_grad'],\n",
    ")\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/climaX-finetune/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:606: UserWarning: Checkpoint directory /home/allen/ckpts/pyrain-finetune-template exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using /home/allen/.cache/torch_extensions/py38_cu113 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/allen/.cache/torch_extensions/py38_cu113/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name | Type            | Params\n",
      "-----------------------------------------\n",
      "0 | net  | ClimaXRainBench | 111 M \n",
      "-----------------------------------------\n",
      "111 M     Trainable params\n",
      "0         Non-trainable params\n",
      "111 M     Total params\n",
      "222.835   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.3076043128967285 seconds\n",
      "Epoch 0:   4%|▎         | 355/9763 [01:36<42:24,  3.70it/s, loss=1.19, v_num=late, train/precipitationcal=0.631, train/loss=0.631]   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/climaX-finetune/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:48: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/allen/ckpts/pyrain-finetune-template/last.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/allen/ckpts/pyrain-finetune-template/last.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 334/334 [01:08<00:00,  4.85it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "              Test metric                           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "            test/test_loss                       0.9040924310684204\n",
      "        test/test_loss_2192hrs                   1.0321240425109863\n",
      "        test/test_loss_4384hrs                   0.7090372443199158\n",
      "    test/test_loss_precipitationcal              0.6481248736381531\n",
      "test/test_loss_precipitationcal_2192hrs          0.7399079203605652\n",
      "test/test_loss_precipitationcal_4384hrs          0.5082940459251404\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "results = trainer.test(model.cuda(), ckpt_path='/home/allen/ckpts/pyrain-finetune-template/last.ckpt', datamodule=datamodule) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([32, 1, 5, 12])\n",
      "torch.Size([12, 1, 5, 12])\n"
     ]
    }
   ],
   "source": [
    "for pred in model.predictions:\n",
    "    print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare results to the baselines from the [Rainbench](https://arxiv.org/abs/2012.09670) paper:\n",
    "<div>\n",
    "<img src=\"images/baseline.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = RainForecastModule.load_from_checkpoint(checkpoint_path='/home/allen/ckpts_eranino_2000/pyrain-finetune-template/epoch-006.ckpt', net=net, lat2d=lat2d, normalizer=datamodule.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHECKPOINT_HYPER_PARAMS_KEY', 'CHECKPOINT_HYPER_PARAMS_NAME', 'CHECKPOINT_HYPER_PARAMS_TYPE', 'T_destination', '_DeviceDtypeModuleMixin__update_properties', '_LightningModule__check_allowed', '_LightningModule__check_not_nested', '_LightningModule__to_tensor', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__jit_unused_properties__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_apply_batch_transfer_handler', '_automatic_optimization', '_backward_hooks', '_buffers', '_call_batch_hook', '_call_impl', '_current_fx_name', '_device', '_dtype', '_example_input_array', '_forward_hooks', '_forward_pre_hooks', '_get_backward_hooks', '_get_name', '_is_full_backward_hook', '_jit_is_scripting', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_log_hyperparams', '_maybe_warn_non_full_backward_hook', '_metric_attributes', '_modules', '_named_members', '_non_persistent_buffers_set', '_on_before_batch_transfer', '_param_requires_grad_state', '_parameters', '_prevent_trainer_and_dataloaders_deepcopy', '_register_load_state_dict_pre_hook', '_register_sharded_tensor_state_dict_hooks_if_available', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_set_hparams', '_should_prevent_trainer_and_dataloaders_deepcopy', '_slow_forward', '_state_dict_hooks', '_to_hparams_dict', '_trainer', '_truncated_bptt_steps', '_verify_is_manual_optimization', '_version', 'add_module', 'all_gather', 'allow_zero_length_dataloader_with_multiple_devices', 'apply', 'automatic_optimization', 'backward', 'bfloat16', 'buffers', 'categories', 'children', 'clip_gradients', 'configure_callbacks', 'configure_gradient_clipping', 'configure_optimizers', 'configure_sharded_model', 'cpu', 'cuda', 'current_epoch', 'device', 'double', 'dtype', 'dump_patches', 'eval', 'example_input_array', 'extra_repr', 'float', 'forward', 'freeze', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'global_rank', 'global_step', 'half', 'hparams', 'hparams_initial', 'ipu', 'lat', 'lead_times', 'load_from_checkpoint', 'load_pretrained_weights', 'load_state_dict', 'local_rank', 'log', 'log_dict', 'log_grad_norm', 'logger', 'loggers', 'lon', 'loss', 'lr_scheduler_step', 'lr_schedulers', 'manual_backward', 'modules', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'net', 'normalizer', 'on_after_backward', 'on_after_batch_transfer', 'on_before_backward', 'on_before_batch_transfer', 'on_before_optimizer_step', 'on_before_zero_grad', 'on_fit_end', 'on_fit_start', 'on_gpu', 'on_load_checkpoint', 'on_predict_batch_end', 'on_predict_batch_start', 'on_predict_end', 'on_predict_epoch_end', 'on_predict_epoch_start', 'on_predict_model_eval', 'on_predict_start', 'on_save_checkpoint', 'on_test_batch_end', 'on_test_batch_start', 'on_test_end', 'on_test_epoch_end', 'on_test_epoch_start', 'on_test_model_eval', 'on_test_model_train', 'on_test_start', 'on_train_batch_end', 'on_train_batch_start', 'on_train_end', 'on_train_epoch_end', 'on_train_epoch_start', 'on_train_start', 'on_validation_batch_end', 'on_validation_batch_start', 'on_validation_end', 'on_validation_epoch_end', 'on_validation_epoch_start', 'on_validation_model_eval', 'on_validation_model_train', 'on_validation_start', 'optimizer_step', 'optimizer_zero_grad', 'optimizers', 'parameters', 'precision', 'predict_dataloader', 'predict_step', 'predictions', 'prepare_data', 'prepare_data_per_node', 'print', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_load_state_dict_post_hook', 'register_module', 'register_parameter', 'requires_grad_', 'save_hyperparameters', 'set_extra_state', 'setup', 'share_memory', 'state_dict', 'tbptt_split_batch', 'teardown', 'test_dataloader', 'test_epoch_end', 'test_step', 'test_step_end', 'test_step_outputs', 'to', 'to_empty', 'to_onnx', 'to_torchscript', 'toggle_optimizer', 'train', 'train_dataloader', 'trainer', 'training', 'training_epoch_end', 'training_step', 'training_step_end', 'transfer_batch_to_device', 'truncated_bptt_steps', 'type', 'unfreeze', 'untoggle_optimizer', 'use_amp', 'val_dataloader', 'val_step_outputs', 'validation_epoch_end', 'validation_step', 'validation_step_end', 'version', 'weights_lat', 'xpu', 'zero_grad']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method LightningModule.training_step_end of RainForecastModule(\n",
       "  (net): ClimaXRainBench(\n",
       "    (token_embeds): ModuleList(\n",
       "      (0): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (1): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (2): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (3): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (4): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (5): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (6): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (7): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (8): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (9): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (10): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (11): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (12): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (13): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (14): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (15): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (16): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (17): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (18): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (19): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (20): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (21): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (22): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (23): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (24): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (25): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (26): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (27): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "      (28): PatchEmbed(\n",
       "        (proj): Conv2d(1, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (norm): Identity()\n",
       "      )\n",
       "    )\n",
       "    (var_agg): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (lead_time_embed): Linear(in_features=1, out_features=1024, bias=True)\n",
       "    (pos_drop): Dropout(p=0.1, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): Identity()\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): Identity()\n",
       "      )\n",
       "      (1): Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.014)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.014)\n",
       "      )\n",
       "      (2): Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.029)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.029)\n",
       "      )\n",
       "      (3): Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.043)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.043)\n",
       "      )\n",
       "      (4): Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.057)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.057)\n",
       "      )\n",
       "      (5): Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.071)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.071)\n",
       "      )\n",
       "      (6): Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.086)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.086)\n",
       "      )\n",
       "      (7): Block(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (proj_drop): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls1): Identity()\n",
       "        (drop_path1): DropPath(drop_prob=0.100)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (act): GELU(approximate=none)\n",
       "          (drop1): Dropout(p=0.1, inplace=False)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (drop2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ls2): Identity()\n",
       "        (drop_path2): DropPath(drop_prob=0.100)\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (1): GELU(approximate=none)\n",
       "      (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (3): GELU(approximate=none)\n",
       "      (4): Linear(in_features=1024, out_features=1, bias=True)\n",
       "    )\n",
       "    (time_agg): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dir(test_model))\n",
    "test_model.training_step_end"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "afb56250527054819fcf9d138637959b1d6f2cc63b50ebf691b81e000c3d0b62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.16 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
